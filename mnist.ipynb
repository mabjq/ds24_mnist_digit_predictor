{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Dependencies\n",
    "\n",
    "This section loads all necessary libraries for data handling, visualization, model training, and image augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from imgaug import augmenters as iaa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Split MNIST Data\n",
    "\n",
    "This section splits the loaded MNIST dataset into training, validation, and test sets, and normalizes the pixel values to the range [0, 1] for better model performance. The splits are performed in two steps to create distinct subsets for training, validation, and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "mnist = fetch_openml('mnist_784', version=1, cache=True, as_frame=False)\n",
    "X, y = mnist[\"data\"], mnist[\"target\"].astype(np.uint8)\n",
    "\n",
    "# Description\n",
    "print(mnist.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividing by 255 to scale the data from [0, 255] to [0, 1] The method is effective and fast for normalization image data and preserves the proportional intensity of pixels, aligns with the uniform range of MNIST, and ensures consistency across all sets without requiring statistical fitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train_val, test\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=10000, random_state=42)\n",
    "\n",
    "# Split train, val\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=10000, random_state=42)\n",
    "\n",
    "# Normalize all sets\n",
    "X_train = X_train / 255.0\n",
    "X_val = X_val / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_train_val = X_train_val / 255.0\n",
    "\n",
    "print(f\"Training data: {X_train.shape}, Validation data: {X_val.shape}, Test data: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for Training and Evaluation\n",
    "\n",
    "This section defines a utility function to train a model, evaluate its performance on a specified dataset (validation or test), and visualize the results using accuracy, classification report, and a confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function - train & evaluate \n",
    "def train_and_evaluate(model, X_train, y_train, X_val, y_val, set_name=\"Validation\"):\n",
    "    \"\"\"\n",
    "    Train and validate a model.\n",
    "    Parameters:\n",
    "    - model: Sklearn-model\n",
    "    - X_train, y_train: Training data\n",
    "    - X_val, y_val: Validation data\n",
    "    - set_name: Name of validation data\n",
    "    \"\"\"\n",
    "    # Fit model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # Accuracy\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    print(f\"Accuracy on {set_name} set: {acc:.4f}\")\n",
    "    \n",
    "    # Precision, recall, f1-score\n",
    "    print(f\"\\nClassification Report ({set_name} set):\")\n",
    "    print(classification_report(y_val, y_pred))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.title(f\"Confusion Matrix ({set_name} set)\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()\n",
    "    \n",
    "    return model, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "This section explores the MNIST dataset to understand its structure and characteristics by visualizing sample digits, check the distribution of classes, compute mean images per digit, analyze pixel intensity distribution, and examine pixel variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check random labels & numbers\n",
    "plt.imshow(X_train[4].reshape(28, 28), cmap='gray_r')\n",
    "plt.title(f\"Label: {y_train[4]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of classes\n",
    "print(\"Class distribution in training set:\", np.bincount(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean pixel value per class (0-9)\n",
    "mean_images = np.zeros((10, 28, 28))\n",
    "for i in range(10):\n",
    "    mean_images[i] = np.mean(X_train[y_train == i], axis=0).reshape(28, 28)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(mean_images[i], cmap='gray_r')\n",
    "    plt.title(f\"Mean {i}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of pixel values\n",
    "plt.hist(X_train.flatten(), bins=50, range=(0, 1))\n",
    "plt.title(\"Pixel Intensity Distribution\")\n",
    "plt.xlabel(\"Normalized Intensity\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pixel variance\n",
    "pixel_variance = np.var(X_train, axis=0)\n",
    "plt.imshow(pixel_variance.reshape(28, 28), cmap='hot')\n",
    "plt.title(\"Pixel Variance Across Images\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression - Baseline\n",
    "\n",
    "This section implements Logistic Regression as a baseline model to establish a performance benchmark for the MNIST classification. Pipeline with GridSearchCV is used to tune the regularization parameter (C), evaluate the best model on the validation set, and save it for future use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline for LR\n",
    "pipeline_lr = Pipeline([\n",
    "    (\"logistic_regression\", LogisticRegression(max_iter=1000, random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid \n",
    "param_grid = {\n",
    "    \"logistic_regression__C\": [0.01, 0.05, 0.2, 0.5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline_lr, param_grid, cv=3, scoring=\"accuracy\", n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best accuracy:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model\n",
    "best_lr_model = grid_search.best_estimator_\n",
    "lr_model, lr_acc = train_and_evaluate(best_lr_model, X_train, y_train, X_val, y_val, set_name=\"Validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "joblib.dump(lr_model, \"logistic_regression_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine\n",
    "\n",
    "This section implements a Support Vector Machine (SVM) with an RBF kernel as a more advanced model. Pipeline with GridSearchCV is used to tune the hyperparameters C (regularization) and gamma (kernel coefficient), evaluate the best model on the validation set, and save it. SVM was chosen due to its strong performance on image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline for SVM\n",
    "pipeline_svm = Pipeline([\n",
    "    (\"svm\", SVC(kernel=\"rbf\", random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid \n",
    "param_grid_svm = {\n",
    "    \"svm__C\": [1.0, 5.0, 10.0],\n",
    "    \"svm__gamma\": [\"scale\", 0.01, 0.1]\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run GridSearchCV\n",
    "grid_search_svm = GridSearchCV(pipeline_svm, param_grid_svm, cv=3, scoring=\"accuracy\", n_jobs=-1, verbose=1)\n",
    "grid_search_svm.fit(X_train, y_train)\n",
    "\n",
    "# Print results\n",
    "print(\"Best parameters for SVM:\", grid_search_svm.best_params_)\n",
    "print(\"Best accuracy for SVM:\", grid_search_svm.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best model\n",
    "best_svm_model = grid_search_svm.best_estimator_\n",
    "svm_model, svm_acc = train_and_evaluate(best_svm_model, X_train, y_train, X_val, y_val, set_name=\"Validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model \n",
    "joblib.dump(svm_model, \"svm_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "This section implements a Random Forest classifier as another model for MNIST digit classification. Pipeline with GridSearchCV is used to tune hyperparameters like the number of trees (n_estimators), maximum depth, and minimum samples per split. The best model is evaluated on the validation set and saved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline for Random Forest\n",
    "pipeline_rf = Pipeline([\n",
    "    (\"random_forest\", RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "param_grid_rf = {\n",
    "    \"random_forest__n_estimators\": [100, 200, 300],\n",
    "    \"random_forest__max_depth\": [None, 20, 30],\n",
    "    \"random_forest__min_samples_split\": [2, 5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run GridSearchCV\n",
    "grid_search_rf = GridSearchCV(pipeline_rf, param_grid_rf, cv=3, scoring=\"accuracy\", n_jobs=-1, verbose=1)\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Print results\n",
    "print(\"Best parameters for Random Forest:\", grid_search_rf.best_params_)\n",
    "print(\"Best accuracy for Random Forest:\", grid_search_rf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model \n",
    "best_rf_model = grid_search_rf.best_estimator_\n",
    "rf_model, rf_acc = train_and_evaluate(best_rf_model, X_train, y_train, X_val, y_val, set_name=\"Validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "joblib.dump(rf_model, \"random_forest_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble\n",
    "\n",
    "This section creates an ensemble model using a VotingClassifier to combine the Support Vector Machine (SVM) and Random Forest models hoping the strengths of both models (non-linear kernel-based and tree-based approaches) can improve performance on the MNIST classification. The model is evaluated on the validation set and saved for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best models\n",
    "best_svm = grid_search_svm.best_estimator_\n",
    "best_rf = grid_search_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ensemble\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[\n",
    "        (\"svm\", best_svm),\n",
    "        (\"rf\", best_rf)\n",
    "    ],\n",
    "    voting=\"hard\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate\n",
    "ensemble_model, ensemble_acc = train_and_evaluate(ensemble, X_train, y_train, X_val, y_val, set_name=\"Validation\")\n",
    "\n",
    "print(f\"Ensemble accuracy: {ensemble_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "joblib.dump(ensemble_model, \"ensemble_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Model Results\n",
    "\n",
    "This section compares the accuracies of the Logistic Regression, Random Forest, SVM, and Ensemble models to evaluate their performance and guides the final model selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "results = {\n",
    "    \"Logistic Regression\": lr_acc,\n",
    "    \"Random Forest\": rf_acc,\n",
    "    \"SVM\": svm_acc,\n",
    "    \"Ensemble\": ensemble_acc\n",
    "}\n",
    "print(\"Model comparison:\")\n",
    "for model, acc in results.items():\n",
    "    print(f\"{model}: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation and Retraining of SVM\n",
    "\n",
    "This section applies augmentation to the training and validation set to improve the SVM model's robustness. This is especially important for real-world applications like camera feed digit recognition. Augmentation includes rotations, translations, and elastic deformations. The retrained model is evaluated on the test set and the augmented model is saved for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape X_train_val for augmentation\n",
    "X_train_val_reshaped = X_train_val.reshape(-1, 28, 28)\n",
    "\n",
    "# Define augmentation sequence\n",
    "seq = iaa.Sequential([\n",
    "    iaa.Affine(rotate=(-10, 10)),  # Rotate ±10 degrees\n",
    "    iaa.Affine(translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)}),  # Shifting\n",
    "    iaa.ElasticTransformation(alpha=0.5, sigma=0.25),  # Elastic deformation\n",
    "])\n",
    "\n",
    "# Apply augmentation (3 extra versions per image + original)\n",
    "augmented_images = []\n",
    "augmented_labels = []\n",
    "for img, label in zip(X_train_val_reshaped, y_train_val):\n",
    "    aug_images = [img] + seq.augment_images([img] * 3)  \n",
    "    augmented_images.extend(aug_images)\n",
    "    augmented_labels.extend([label] * 4)\n",
    "\n",
    "augmented_images = np.array(augmented_images)\n",
    "augmented_labels = np.array(augmented_labels)\n",
    "\n",
    "# Flatten augmented data for SVC\n",
    "X_train_val_augmented = augmented_images.reshape(-1, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain SVM with augmented data\n",
    "print(\"Retraining SVM model on augmented train_val set...\")\n",
    "svm_model_augmented = SVC(kernel=\"rbf\", C=10.0, gamma=\"scale\", random_state=42)  \n",
    "svm_model_augmented.fit(X_train_val_augmented, augmented_labels)\n",
    "\n",
    "# Evaluate on test set\n",
    "X_test_flat = X_test.reshape(-1, 784)\n",
    "y_pred_test = svm_model_augmented.predict(X_test_flat)\n",
    "test_acc_augmented = accuracy_score(y_test, y_pred_test)\n",
    "print(f\"Accuracy on test set with augmented model: {test_acc_augmented:.4f}\")\n",
    "\n",
    "# Plot confusion matrix for test set\n",
    "cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title(\"Confusion Matrix (Test set with Augmented Model)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "# Save the retrained augmented model\n",
    "joblib.dump(svm_model_augmented, \"svm_augmented_model.pkl\")\n",
    "print(\"Augmented model saved as svm_augmented_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "## Summary of Findings\n",
    "This project developed a digit classifier (handwritten) using the MNIST dataset, with the goal of deploying it for real-world camera feed recognition. \n",
    "The explored models and techniques were:\n",
    "\n",
    "- **Baseline Model:** Logistic Regression achieved a validation accuracy of 0.9233, serving as a benchmark.\n",
    "- **Advanced Models:** Random Forest and SVM outperformed the baseline, with validation accuracies of 0.9713 and 0.9852, respectively. SVM with an RBF kernel was the best individual model, demonstrating a strength in capturing non-linear patterns in image data.\n",
    "- **Ensemble:** Combining SVM and Random Forest with a VotingClassifier gave an accuracy of 0.9782, slightly below SVM's performance and had limited benefits from ensembling in this case.\n",
    "- **Augmentation and Retraining:** To improve robustness for real-world applications, augmentation were applied (rotations, translations, elastic deformations) to the training+validation set, increasing the dataset size by x4. Retraining the SVM on this augmented data resulted in a test accuracy of 0.9879, a strong result that aligns with top SVM benchmarks on MNIST that I've found (~0.986–0.99) and this while requiring minimal tuning effort.\n",
    "\n",
    "## Key Takeaways\n",
    "- SVM with an RBF kernel was the most effective model, achieving 0.9852 on the validation set and 0.9879 on the test set after augmentation.\n",
    "- Augmentation is critical for improving generalization, simulating real-world variations like camera angles and handwriting distortions, and preparing the model for deployment in apps like Streamlit.\n",
    "- The use of moderate hyperparameter ranges, balanced augmentation and still achieve a competitive result, demonstrates the efficiency of SVM for this task.\n",
    "\n",
    "## Next Steps\n",
    "- **Deployment:** Deploy the augmented SVM model (`svm_augmented_model.pkl`) to Streamlit for real-time digit recognition from drawing canvas, image upload and camera feed.\n",
    "- **Further Improvements could be:** Explore additional augmentations (noise injection, scaling) or soft voting in the ensemble to potentially boost performance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
